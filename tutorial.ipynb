{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Reinforcement Learning (GBRL)\n",
    "GBRL is a Python-based GBT library designed and optimized for reinforcement learning (RL).\n",
    "GBRL is designed to be integrated in popular RL python libraries as part of the standard RL training loop.  \n",
    "\n",
    "## GBRL Design\n",
    "The standard GBT supervised learning training procedure for K boosting iterations on a given set on inputs x and targets y is as follows:  \n",
    "***For K boosting iterations***   \n",
    "1. Generate predictions using current GBT ensemble.\n",
    "2. Calculate loss L(y, predictions).\n",
    "3. Calculate gradients of the loss function w.r.t predictions  \n",
    "4. Fit a binary decision tree on the gradients and add it to the ensemble.\n",
    "5. Repeat from step 1.\n",
    "\n",
    "The training procedure is typically done E2E within a GBT framework and is optimized for pre-defined loss functions. GBRL modifies this procedure to seemingly integrate within RL loops by:\n",
    "- Outsourcing gradient calculation to autograd frameworks.\n",
    "- Incremental learning by performing a single boosting iteration on a given data batch containing pairs of states/observations and gradients.  \n",
    "\n",
    "\n",
    "## Get Started with GBRL\n",
    "This is a quick tutorial demonstrating usage examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training Procedure \n",
    "***Note: standard training procedure is based on PyTorch***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gbrl.gbrl_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mse_loss \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgbrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingTrees, cuda_available, ParametricActor\n",
      "File \u001b[0;32m/.autodirect/swgwork/bfuhrer/projects/gbrl/nvlabs/gbrl/gbrl/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mac_gbrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ActorCritic, GaussianActor, ContinuousCritic,\n\u001b[1;32m      4\u001b[0m                    DiscreteCritic, ParametricActor)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgbt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingTrees\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgbrl_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GBRL\n",
      "File \u001b[0;32m/.autodirect/swgwork/bfuhrer/projects/gbrl/nvlabs/gbrl/gbrl/ac_gbrl.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mth\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgbrl_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (GBTWrapper, SeparateActorCriticWrapper,\n\u001b[1;32m     10\u001b[0m                            SharedActorCriticWrapper, numerical_dtype)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgbt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingTrees\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_optimizer, clip_grad_norm\n",
      "File \u001b[0;32m/.autodirect/swgwork/bfuhrer/projects/gbrl/nvlabs/gbrl/gbrl/gbrl_wrapper.py:18\u001b[0m\n\u001b[1;32m     13\u001b[0m categorical_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS128\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgbrl_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GBRL\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_array\u001b[39m(arr: np\u001b[38;5;241m.\u001b[39marray)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39marray]:\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Formats numpy array for C++ GBRL.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gbrl.gbrl_cpp'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import gymnasium as gym \n",
    "\n",
    "from sklearn import datasets\n",
    "from torch.nn.functional import mse_loss \n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gbrl import GradientBoostingTrees, cuda_available, ParametricActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incremental learning dataset\n",
    "X_numpy, y_numpy = datasets.load_diabetes(return_X_y=True, as_frame=False, scaled=False)\n",
    "out_dim = 1 if len(y_numpy.shape) == 1  else  y_numpy.shape[1]\n",
    "if out_dim == 1:\n",
    "    y_numpy = y_numpy[:, np.newaxis]\n",
    "\n",
    "X, y = th.tensor(X_numpy, dtype=th.float32), th.tensor(y_numpy, dtype=th.float32)\n",
    "# CUDA is not deterministic\n",
    "device = 'cuda' if cuda_available else 'cpu'\n",
    "\n",
    "# initializing model parameters\n",
    "tree_struct = {'max_depth': 4, \n",
    "               'n_bins': 256,\n",
    "               'min_data_in_leaf': 0,\n",
    "               'par_th': 2,\n",
    "               'grow_policy': 'oblivious'\n",
    "        }\n",
    "\n",
    "optimizer = { 'algo': 'SGD',\n",
    "              'lr': 1.0,\n",
    "            }\n",
    "gbrl_params = {\n",
    "               \"split_score_func\": \"Cosine\",\n",
    "               \"generator_type\": \"Quantile\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting GBRL device to cuda\n",
      "Setting policy optimizer indices: 0->1\n"
     ]
    }
   ],
   "source": [
    "# setting up model\n",
    "gbt_model = GradientBoostingTrees(\n",
    "                    output_dim=out_dim,\n",
    "                    tree_struct=tree_struct,\n",
    "                    optimizer=optimizer,\n",
    "                    gbrl_params=gbrl_params,\n",
    "                    verbose=0,\n",
    "                    device=device)\n",
    "gbt_model.set_bias(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting iteration: 1 RMSE loss: 54.45128631591797\n",
      "Boosting iteration: 2 RMSE loss: 45.48917007446289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting iteration: 3 RMSE loss: 40.836509704589844\n",
      "Boosting iteration: 4 RMSE loss: 37.73506546020508\n",
      "Boosting iteration: 5 RMSE loss: 36.77689743041992\n",
      "Boosting iteration: 6 RMSE loss: 35.08983612060547\n",
      "Boosting iteration: 7 RMSE loss: 33.8794059753418\n",
      "Boosting iteration: 8 RMSE loss: 32.981075286865234\n",
      "Boosting iteration: 9 RMSE loss: 32.36515426635742\n",
      "Boosting iteration: 10 RMSE loss: 31.6733341217041\n"
     ]
    }
   ],
   "source": [
    "# training for 10 epochs\n",
    "n_epochs = 10\n",
    "for _ in range(n_epochs):\n",
    "    # forward pass - setting requires_grad=True is mandatory for training\n",
    "    # y_pred is a torch tensor\n",
    "    y_pred = gbt_model(X, requires_grad=True)\n",
    "    # calculate loss - we must scale pytorch's mse loss function by 0.5 to get the correct MSE gradient\n",
    "    loss = 0.5*mse_loss(y_pred, y) \n",
    "    loss.backward()\n",
    "    # perform a boosting step\n",
    "    gbt_model.step(X)\n",
    "    print(f\"Boosting iteration: {gbt_model.get_iteration()} RMSE loss: {loss.sqrt()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBT work with per-sample gradients but pytorch typically calculates the expected loss. GBRL internally multiplies the gradients with the number of samples when calling the step function. Therefore, when working with pytorch losses and multi-output targets one should take this into consideration.  \n",
    "For example:\n",
    "1. When using a summation reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting GBRL device to cuda\n",
      "Setting policy optimizer indices: 0->1\n",
      "Boosting iteration: 1 RMSE loss: 54.45128631591797\n",
      "Boosting iteration: 2 RMSE loss: 45.48917007446289\n",
      "Boosting iteration: 3 RMSE loss: 40.836509704589844\n",
      "Boosting iteration: 4 RMSE loss: 37.73506546020508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting iteration: 5 RMSE loss: 36.77689743041992\n",
      "Boosting iteration: 6 RMSE loss: 35.08983612060547\n",
      "Boosting iteration: 7 RMSE loss: 33.87940216064453\n",
      "Boosting iteration: 8 RMSE loss: 32.981075286865234\n",
      "Boosting iteration: 9 RMSE loss: 32.36515426635742\n",
      "Boosting iteration: 10 RMSE loss: 31.6733341217041\n"
     ]
    }
   ],
   "source": [
    "gbt_model = GradientBoostingTrees(\n",
    "                    output_dim=out_dim,\n",
    "                    tree_struct=tree_struct,\n",
    "                    optimizer=optimizer,\n",
    "                    gbrl_params=gbrl_params,\n",
    "                    verbose=0,\n",
    "                    device=device)\n",
    "gbt_model.set_bias(y)\n",
    "# continuing training 10  epochs using a sum reduction\n",
    "n_epochs = 10\n",
    "for _ in range(n_epochs):\n",
    "    y_pred = gbt_model(X, requires_grad=True)\n",
    "    # we divide the loss by the number of samples to compensate for GBRL's built-in multiplication by the same value   \n",
    "    loss = 0.5*mse_loss(y_pred, y, reduction='sum') / len(y_pred) \n",
    "    loss.backward()\n",
    "    # perform a boosting step\n",
    "    gbt_model.step(X)\n",
    "    print(f\"Boosting iteration: {gbt_model.get_iteration()} RMSE loss: {loss.sqrt()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When working with multi-dimensional outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting GBRL device to cuda\n",
      "Setting policy optimizer indices: 0->2\n",
      "Boosting iteration: 1 RMSE loss: 54.45128631591797\n",
      "Boosting iteration: 2 RMSE loss: 45.48917007446289\n",
      "Boosting iteration: 3 RMSE loss: 40.836509704589844\n",
      "Boosting iteration: 4 RMSE loss: 37.73506546020508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting iteration: 5 RMSE loss: 36.77689743041992\n",
      "Boosting iteration: 6 RMSE loss: 35.08983612060547\n",
      "Boosting iteration: 7 RMSE loss: 33.87940216064453\n",
      "Boosting iteration: 8 RMSE loss: 32.981075286865234\n",
      "Boosting iteration: 9 RMSE loss: 32.365150451660156\n",
      "Boosting iteration: 10 RMSE loss: 31.6733341217041\n"
     ]
    }
   ],
   "source": [
    "y_multi = th.concat([y, y], dim=1)\n",
    "out_dim = y_multi.shape[1]\n",
    "gbt_model = GradientBoostingTrees(\n",
    "                    output_dim=out_dim,\n",
    "                    tree_struct=tree_struct,\n",
    "                    optimizer=optimizer,\n",
    "                    gbrl_params=gbrl_params,\n",
    "                    verbose=0,\n",
    "                    device=device)\n",
    "gbt_model.set_bias(y_multi)\n",
    "# continuing training 10  epochs using a sum reduction\n",
    "n_epochs = 10\n",
    "for _ in range(n_epochs):\n",
    "    y_pred = gbt_model(X, requires_grad=True)\n",
    "    # we multiply the loss by the output dimension to compensate for pytorch's mean reduction for MSE loss that averages across all dimensions.\n",
    "    # this step is necessary to get the correct loss gradient - however the loss value itself is correct\n",
    "    loss = 0.5*mse_loss(y_pred, y_multi) * out_dim\n",
    "    loss.backward()\n",
    "    # perform a boosting step\n",
    "    gbt_model.step(X)\n",
    "    print(f\"Boosting iteration: {gbt_model.get_iteration()} RMSE loss: {(loss / out_dim).sqrt()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL using GBRL\n",
    "Now that we have seen how GBRL is trained using incremental learning and PyTorch we can use it within an RL training loop\n",
    "\n",
    "Let's start by training a simple Reinforce Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, gamma):\n",
    "    returns = []\n",
    "    running_g = 0.0\n",
    "    for reward in rewards[::-1]:\n",
    "        running_g = reward + gamma * running_g\n",
    "        returns.insert(0, running_g)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting GBRL device to cpu\n",
      "Setting policy optimizer indices: 0->2\n",
      "Episode 0 - boosting iteration: 0 episodic return: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 - boosting iteration: 10 episodic return: 22.280000686645508\n",
      "Episode 200 - boosting iteration: 20 episodic return: 29.299999237060547\n",
      "Episode 300 - boosting iteration: 30 episodic return: 32.63999938964844\n",
      "Episode 400 - boosting iteration: 40 episodic return: 42.720001220703125\n",
      "Episode 500 - boosting iteration: 50 episodic return: 51.540000915527344\n",
      "Episode 600 - boosting iteration: 60 episodic return: 72.80000305175781\n",
      "Episode 700 - boosting iteration: 70 episodic return: 78.9000015258789\n",
      "Episode 800 - boosting iteration: 80 episodic return: 112.83999633789062\n",
      "Episode 900 - boosting iteration: 90 episodic return: 129.72000122070312\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
    "num_episodes = 1000\n",
    "gamma = 0.997\n",
    "optimizer = { 'algo': 'SGD',\n",
    "              'lr': 0.01,\n",
    "            }\n",
    "\n",
    "bias = np.zeros(env.action_space.n, dtype=np.single)\n",
    "agent = ParametricActor(\n",
    "                    output_dim=env.action_space.n,\n",
    "                    tree_struct=tree_struct,\n",
    "                    policy_optimizer=optimizer,\n",
    "                    gbrl_params=gbrl_params,\n",
    "                    verbose=0,\n",
    "                    bias=bias, \n",
    "                    device='cpu')\n",
    "\n",
    "\n",
    "update_every = 10\n",
    "\n",
    "rollout_buffer = {'actions': [], 'obs': [], 'returns': []}\n",
    "for episode in range(num_episodes):\n",
    "    # gymnasium v26 requires users to set seed while resetting the environment\n",
    "    obs, info = wrapped_env.reset(seed=0)\n",
    "    rollout_buffer['rewards'] = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_logits = agent(obs)\n",
    "        action = Categorical(logits=action_logits).sample()\n",
    "        action_numpy = action.cpu().numpy()\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = wrapped_env.step(action_numpy.squeeze())\n",
    "        rollout_buffer['rewards'].append(reward)\n",
    "        rollout_buffer['obs'].append(obs)\n",
    "        rollout_buffer['actions'].append(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    rollout_buffer['returns'].extend(calculate_returns(rollout_buffer['rewards'], gamma))\n",
    "\n",
    "\n",
    "    if episode % update_every == 0 and episode > 0:\n",
    "        returns = th.tensor(rollout_buffer['returns'])\n",
    "        actions = th.cat(rollout_buffer['actions'])\n",
    "        # input to model can be either a torch tensor or a numpy ndarray\n",
    "        observations = np.stack(rollout_buffer['obs'])\n",
    "        # model update\n",
    "        action_logits = agent(observations, requires_grad=True)\n",
    "        dist = Categorical(logits=action_logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        # calculate reinforce loss with subtracted baseline\n",
    "        loss = -(log_probs*(returns - returns.mean())).mean()\n",
    "        loss.backward()\n",
    "        grads = agent.step(observations)\n",
    "        rollout_buffer = {'actions': [], 'obs': [], 'returns': []}\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} - boosting iteration: {agent.get_iteration()} episodic return: {np.mean(wrapped_env.return_queue)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Manually Calculated Gradients\n",
    "Alternatively GBRL can use manually calculated gradients.  Calling the `predict` method instead of the `__call__` method, returns a numpy array instead of a PyTorch tensor. Autograd libraries or manual calculations can be used to calculate gradients.  \n",
    "Fitting manually calculated gradients is done using the `_model.step` method that receives numpy arrays. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting GBRL device to cuda\n",
      "Setting policy optimizer indices: 0->1\n",
      "Boosting iteration: 1 RMSE loss: 54.451285094616374\n",
      "Boosting iteration: 2 RMSE loss: 45.48916999877324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting iteration: 3 RMSE loss: 40.83651082662459\n",
      "Boosting iteration: 4 RMSE loss: 37.73506439069844\n",
      "Boosting iteration: 5 RMSE loss: 36.77689669772262\n",
      "Boosting iteration: 6 RMSE loss: 35.089837631524226\n",
      "Boosting iteration: 7 RMSE loss: 33.87940389697403\n",
      "Boosting iteration: 8 RMSE loss: 32.98107514282689\n",
      "Boosting iteration: 9 RMSE loss: 32.365154094608144\n",
      "Boosting iteration: 10 RMSE loss: 31.67333523835015\n"
     ]
    }
   ],
   "source": [
    "# initializing model parameters\n",
    "tree_struct = {'max_depth': 4, \n",
    "               'n_bins': 256,\n",
    "               'min_data_in_leaf': 0,\n",
    "               'par_th': 2,\n",
    "               'grow_policy': 'oblivious'\n",
    "        }\n",
    "\n",
    "optimizer = { 'algo': 'SGD',\n",
    "              'lr': 1.0,\n",
    "            }\n",
    "gbrl_params = {\n",
    "               \"split_score_func\": \"Cosine\",\n",
    "               \"generator_type\": \"Quantile\"\n",
    "                }\n",
    "# setting up model\n",
    "gbt_model = GradientBoostingTrees(\n",
    "                    output_dim=1,\n",
    "                    tree_struct=tree_struct,\n",
    "                    optimizer=optimizer,\n",
    "                    gbrl_params=gbrl_params,\n",
    "                    verbose=0,\n",
    "                    device=device)\n",
    "# works with numpy arrays as well as PyTorch tensors\n",
    "gbt_model.set_bias(y_numpy)\n",
    "\n",
    "# training for 10 epochs\n",
    "n_epochs = 10\n",
    "for _ in range(n_epochs):\n",
    "    # y_pred is a numpy array\n",
    "    y_pred = gbt_model.predict(X_numpy)\n",
    "    loss = np.sqrt(0.5*((y_pred - y_numpy)**2).mean()) \n",
    "    grads = y_pred - y_numpy\n",
    "    # perform a boosting step\n",
    "    gbt_model._model.step(X_numpy, grads)\n",
    "    print(f\"Boosting iteration: {gbt_model.get_iteration()} RMSE loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "GBRL supports training multiple boosting iterations with targets similar to other GBT libraries. This is done using the `fit` method.  \n",
    "***Note: only the RMSE loss function is supported for the `fit` method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting GBRL device to cuda\n",
      "Setting policy optimizer indices: 0->1\n",
      "0 - MultiRMSE: 45.4892\n",
      "1 - MultiRMSE: 40.8918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 - MultiRMSE: 38.3409\n",
      "3 - MultiRMSE: 36.839\n",
      "4 - MultiRMSE: 35.6598\n",
      "5 - MultiRMSE: 34.7947\n",
      "6 - MultiRMSE: 33.7887\n",
      "7 - MultiRMSE: 33.0885\n",
      "8 - MultiRMSE: 32.3866\n",
      "9 - MultiRMSE: 31.6777\n"
     ]
    }
   ],
   "source": [
    "gbt_model = GradientBoostingTrees(\n",
    "                    output_dim=1,\n",
    "                    tree_struct=tree_struct,\n",
    "                    optimizer=optimizer,\n",
    "                    gbrl_params=gbrl_params,\n",
    "                    verbose=1,\n",
    "                    device=device)\n",
    "final_loss = gbt_model.fit(X_numpy, y_numpy, iterations=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
